# -*- coding: utf-8 -*-
"""Machine Learning Assignment1 Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-FWfaZOF44UEZznVBrXwQ8Nzpv17z96A
"""

# Commented out IPython magic to ensure Python compatibility.
#####ALL THE IMPORTS AND LIBRARIES NEEDED FOR THE ASSIGNMENT###################################################

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, confusion_matrix,classification_report
import random

############THIS CONVERTS THE DATA FROM "EXCEL SHEET ONLY TO A PANDAS DATAFRAME"
df = pd.read_excel("IndiaCOVIDStatistics.xlsx")

df.columns

"""## ***DATA EXPLORATION*** """

############################### NOTE ##################################################

###   THE FOLLOWING CELLS NEED NOT BE RUN BY THE EVALUATOR, THESE ARE DATA EXPLORATION DONE TO
###  ASSIST ASSIGNMENT SOLVING. THE EVALUATOR CAN DIRECTLY SKIP THE THE NEXT SECTION. THE PLOTS OBTAINED IN THIS SECTION ARE SUBMITTED



df1.columns

#df1.columns = ['State', 'ConfirmedIndianNational',
       #'ConfirmedForeignNational', 'Cured', 'Deaths', 'Confirmed']

#a = df1["State"].value_counts()

plt.figure(figsize = (15,20))

sns.set_context(context = 'notebook',font_scale = 1.6)
sns.set_style(style = 'whitegrid')

ax = sns.barplot(x = df1["State"], y = df1["Deaths"])
ax.set_xticklabels(ax.get_xticklabels(),rotation = 45,ha = 'right',fontsize = 15)
plt.title("State wise Deaths")

plt.figure(figsize = (12,6))
sns.distplot(df["Confirmed"], bins  = 30)
plt.title("The distrubition of Confirmed cases")

plt.figure(figsize = (12,8))
sns.distplot(df["Cured"], bins = 100)
plt.title("The distrubition of Indian Confirmed cases")

plt.figure(figsize = (12,8))
sns.scatterplot(x = df["Cured"], y = df["Deaths"], hue = hex)
plt.title("The relationship between the Confirmed cases and Deaths")



"""# BUILDING DECISION **TREE**"""

############ ALL THE CELLS BELOW NEED TO BE RUN UNLESS MENTIONED OTHERWISE

df1 = df.drop(labels = ["Date", "Time", "Sno"], axis = 1)

df1.head()

######### CONVERTING THE DATAFRAME INTO A LIST OF DICTIONARIES ########################

global list
dic = (df1.T.to_dict()).values()
list = []
for example in dic:
    list.append(example)

## SHUFFLING THE LIST WHICH WILL HELP WHILE SPLITTIN DATASET TO TRAIN AND TEST

random.seed(4)
random.shuffle(list)

########### TRAIN TEST SPLIT ###########################################################

global test
global train
global validation

train = list[:390]            ## The train dataset will be used to build the decision tree, it will also be used 
                              ## to split into validation and grow set in order to choose the hyperparameters

test = list[390:]            ## The test dataset will be used to test and report the final accuracy of model
                              ## No where in this notebook we use the test set to GROW the Decision Tree


validation = train[355:]     ## VALIDATION SET. NOTE THIS VALIDATION SET IS ONLY USED FOR PRUNING
                              ## There is a separate function that splits the complete training set into KFold 10 compartments for model evaluation
train = train[:355]

def find_mean(s):

  """
    This function finds the mean value for a sample set. The set has to be numeric in natures

    params:
        s (list):
          list of numeric elements

    returns:
        mean of the elements

  """

  mean = 0;

  for value in s:
    mean += value["Deaths"]
  
  if len(s) != 0:

    mean /= len(s)


  return mean

def root_meansquare_error(s, prediction):

    """
      This function calculates the variance of a set s with a fixed value

      params:
        s (list):
        prediction (int):
           value about which we need to calculate the room means square error

      returns:
          sum of squared difference between each element in s and prediction

    """
    error = 0;
    for value in s:
        error += (prediction - value["Deaths"])**2
        
    return error

def rmse_of_set(list):

    """
      returns:
        rmse_error of a list

    """
    return root_meansquare_error(list, find_mean(list))

def select_best_split(attribute, split_candidate_list, data_list):

    """
      This function calculates the best split for a particualr function from 
      all possible splits. This takes the candidate split and based on a heuristic
      (explained in report) computes the best split with regrads to the given attribute

      params:
          attrbute (string):
              the attriute for which best split is needed

          split_candidate_list (list):
                list containing all the candidates for binary split

          data_list (list):
                list of data points in dictionary format

      returns:
           a dictionary containing: 1. The best value for split 2. The score obtained 3: the child left and right data points

    """
    
    split_rmse = {} ## This dictionary maintains the root mean square error for each split
                    ##ideally we select the split with thr least mean squared error
    b_left = []
    b_right = []
    b_score = float("inf")
    

    for candidate in split_candidate_list:
        
        left_branch = []    
        right_branch = []

        for example in data_list: ## Here list is the list of of all data points
            if attribute == "State/UnionTerritory":
                if example[attribute] == candidate:
                    right_branch.append(example)
                else:
                    left_branch.append(example)
            else:
                if example[attribute] < candidate:
                    left_branch.append(example)
                else:
                    right_branch.append(example)

        split_rmse[candidate] = rmse_of_set(left_branch) + rmse_of_set(right_branch)
        if(split_rmse[candidate] < b_score):
            b_score = split_rmse[candidate]
            b_left = left_branch
            b_right = right_branch
    
    
    #return split_rmse      
    return {'value': min(split_rmse, key = split_rmse.get), 'score': min(split_rmse.values()) , 'groups': (b_left, b_right)}

def select_node(data_list):

    """
      This function calls "split_candidate_list" for each attriute.
      It then calculates the best attribute and best value of that attribute
      to split on for the next step among all possible attributes and corresponding values. 
      This is the greedy step in the algorithm.

      params:
          data_list (list):
              list containing data points as dictionary

      returns:
        the node that should follow as a dictionary containing: 1. attribute , 2. value, 3. The child data groups

    """
    ##MAXIMUM_TREE_DEPTH = 6
    dataframe = pd.DataFrame.from_dict(list)
    attributes_list = dataframe.drop(labels = "Deaths", axis = 1).columns
    
    best_attribute, best_value, best_score , best_groups = None, None, float("inf"), None
    
    for attribute in attributes_list:
        candidates = split_candidate_list(attribute , dataframe)
        best_split = select_best_split(attribute, candidates , data_list) ## The best split has been seleced
        if best_split['score'] < best_score:
            best_score = best_split['score']
            best_value = best_split['value']
            best_groups = best_split['groups']
            best_attribute = attribute
            
            
    return {'attribute':best_attribute, 'value':best_value, 'groups': best_groups}

def split_candidate_list(attribute , dataframe):

    """
      This function computes all the  candidate values for a binary split 
      with regards to a specific attribute. This candidate list is then computed
      for all attributes

      params:
        attribute (string):
            attribute for which we need the candidates for a split
        dataframe (dataframe):
            the training data in a pandas df structure

      returns:
        split_candidate_list (list):
            a list of all possible binary splits with regards to the given attribute
        

    """
    split_candidate_list = []

    if attribute == "State/UnionTerritory":
        split_candidate_list = dataframe['State/UnionTerritory'].unique()

    else:
        
        data_list = dataframe[attribute].tolist()
        data_array = np.array(data_list)
        sorted_data_array = np.sort(data_array)
        #print(sorted_data_array)
        memory = sorted_data_array[0];
        ##print(memory)
        
        for data in sorted_data_array:
            ##print(data)
            if data > memory:
               ##print(memory)
                #print(data, memory, attribute)
                x = (data + memory)/2
                split_candidate_list.append(x)
                memory = data;

    return split_candidate_list

def to_leaf(s):

    """
    
      This function takes a group of data points to a leaf node based on the mode of classes observed in the set

      returns:
          the most occuring class in the set

    """
    
    outcomes = [element["Deaths"] for element in s]
    
    return max(set(outcomes), key=outcomes.count)

def recursive_split(node, max_depth, min_sample_size, depth):

    """
      This function is the driver function for making the decision tree.
      This function RECURSIVELY finds the best split until it satifies a condition 
      to go to leaf. A depth first search approach. Details of function in report
    
    """
    
    left, right = node['groups']
    #del(node['groups'])
    
    if not left or not right:
        node['left'] = node['right'] = to_leaf(left + right)
        return
    
    if depth >= max_depth:
        node['left'], node['right'] = to_leaf(left), to_leaf(right)
        return
    
    if len(left) <= min_sample_size:
        node['left'] = to_leaf(left)
        
    else:
        node['left'] = select_node(left)
        recursive_split(node['left'], max_depth, min_sample_size, depth+1)
    
    if len(right) <= min_sample_size:
        node['right'] = to_leaf(right)
        
    else:
        node['right'] = select_node(right)
        recursive_split(node['right'], max_depth, min_sample_size, depth+1)
        #print(depth, '\n\n')

def build_regression_tree(data, max_depth, min_sample_size):

    """
      This function computes the root node and then calls the recursive tree
      making function. It returns the root node for the tree.

      params:
        data (list):
            list of data points

        max_depth (int):
            this is a user input ; the desired max depth can be given by the user

        min_sample_size:
            hardcoded to 1, can be changed if pre pruning is required

      returns:
          the root of the final decision tree

      """

    if max_depth == -1:
      max_depth = 100

    
    root_node = select_node(data)
    
    recursive_split(root_node, max_depth, min_sample_size, 1)
    
    return root_node

def predict(node, sample):

    """
      This function predicts the output class for a given sample

      returns:
        The prediction: an output class from 0-5

    """
    
    if node['attribute'] == 'State/UnionTerritory':
        if sample['State/UnionTerritory'] != node['value']:
            if isinstance(node['left'], dict):
                return predict(node['left'], sample)
            else:
                return node['left']
            
        else:
            if isinstance(node['right'], dict):
                return predict(node['right'], sample)
            else:
                return node['right']
    else:
        if sample[node['attribute']] < node['value']:
            if isinstance(node['left'], dict):
                return predict(node['left'], sample)
            else:
                return node['left']
            
        else:
            if isinstance(node['right'], dict):
                return predict(node['right'], sample)
            else:
                return node['right']

def measure_accuracy( predictions, target):

    """
      This outputs the accuracy for a list of predictions given a list of corresponding target variables

      params:
        predictions (list):
          a list of predictions by the decision tree

        target (list):
          a list of true targets for the same data points used to predict with the DT

    """
    
    return accuracy_score(target, predictions)

def Kfold_crossvalidation(train, max_depth):

    """
      This function performs KFold Cross validation to measure the mean accuracy by using 9/10 batches to train and 1 batch to test.

      params:
        train (list):
          training set
        max_depth (int):
          user chosen value

        returns:
          The array of errors: i.e. 10 different error for each step of 10Fold CV

    """
    
    min_samples = 1
    Kfold_CV = KFold(n_splits = 10, random_state=12)
    validation_scores = []
    accuracy_list = []
    
    
    for train_index, validation_index in Kfold_CV.split(train):
        
        predictions_list = []
        target_list = []
        
        #total_error = 0
        root = build_regression_tree(np.array(list)[train_index] , max_depth, min_samples)
        
        for sample in np.array(list)[validation_index]:
            
            predictions_list.append(predict(root, sample))
            
            target_list.append(sample["Deaths"])
            
        accuracy_list.append(measure_accuracy(predictions_list, target_list))
            
           
        
        
    return np.array(accuracy_list)

def prune_step(node, parent_node, right):

    """
      Thus function recursively prunes the leaves of a tree and for each step it, i.e
      for each leaf deletion the tree is evaluated and if the accuracy of the tree exceeds the unpruned 
      tree then the process is continued until otherwise

      params:
        node: 
          this is the node that needs to be deleted

        parent_node:
          the parent of the current node being pruned

        element (bool):
          variable denoting if the current node is to the left or right of the parent node

      """
    
    if not isinstance(node, dict):
        return
    
    if( not isinstance(node['left'], dict) and   not isinstance(node['right'], dict)):
        delete_subtree(node, parent_node, right)
    
    if(isinstance(node['left'], dict)):
        prune_step(node['left'], node,0)
        
    if(isinstance(node['right'], dict)):
        prune_step(node['right'], node, 1)
        
    
    if(parent_node == None):
        return
    
    delete_subtree(node, parent_node, right)

def delete_subtree(node, parent_node, right):

    """
      This function deletes the subtree from the main tree rooting from the node passed
      into the function.
      After each delete it evaluates the tree using the evaluate_tree function and stores the results 
      in a gloabal list. It deletes the subtree and replaces it with a leaf node 
      in the same procedure as explained above. It also evaluates the tree.

    """
    
    #print(parent_node)
    
    elements = node['groups'][0] + node['groups'][1]
    
    prediction = to_leaf(elements)
    
    if(right):
        parent_node['right'] = prediction
    else:
        parent_node['left'] = prediction
    
    evaluate_tree()

def evaluate_tree():

    """
      Evaluates the current level of pruned tree on the validation set.
      It stores the tree and the corresponding score as a dictionary in a gloabal list

    """
    
    predictions_list  = []
    
    target_list = []
    
    for sample in validation:
        
        predictions_list.append(predict(temp_tree, sample))
        target_list.append(sample["Deaths"])
        
    
    temp_2 = temp_tree
    #print('hey')
    tree_performance_list.append({'tree' : temp_2, 'accuracy': measure_accuracy(predictions_list, target_list)})

def evaluate_fulltree(tree):

    """
      This function evaluates the tree that is passed to it using the global validation set

    """
    predictions_list  = []
    
    target_list = []
    
    for sample in validation:
        
        predictions_list.append(predict(tree, sample))
        target_list.append(sample["Deaths"])
        
    return measure_accuracy(predictions_list, target_list)

def find_best_prune( tree_performance_list ):

    """
      iterates through the global list containing performance of each pruned tree

      returns:

        (tuple): containing the best score and the corresponding tree

    """
    
    score = -1
    tree = None
    for a in tree_performance_list:

        if a['accuracy'] > score:
            score = a['accuracy']
            tree = a['tree']
            
    return (score, tree)

def print_tree(node, depth=0):

    #print(node['attribute'], node['value'])
    if isinstance(node, dict):
        if( node['attribute'] == 'State/UnionTerritory'):
            
            print('%s[%s < %s]' % ((depth*2*' ', (node['attribute']), node['value'])))
            
            
        else:
            print('%s[%s < %.3f]' % ((depth*2*' ', (node['attribute']), node['value'])))
        print_tree(node['left'], depth+1)
        print_tree(node['right'], depth+1)
        
    else:
        print('%s[%s]' % ((depth*2*' ', node)))

def confusion_matrix_report(y_test, y_pred):

    mat = confusion_matrix(y_test, y_pred)
    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')

    print(classification_report(y_test, y_pred))

"""# ASSIGNMENTS QUESTIONS"""



############### BUILD THE DECISION TREE ############################################################

MAXIMUM_DEPTH = ###### USER ENTER VALUE ################################

root  = build_regression_tree(list , MAXIMUM_DEPTH, 1)

#The root variable is the root node containing the root of the DT

######################## FIRST QUESTION #####################################################################
#performs KCross validation and outputs the accuracy obtained

MAXIMUM_DEPTH = ###### USER ENTER VALUE ################################3

accuracy_list = Kfold_crossvalidation(train, MAXIMUM_DEPTH)

print(np.mean(accuracy_list))

####################### SECOND QUESTION #######################################################################
## FINDING THE BEST HYPERPARAMETER: DEPTH

depth_accuracy_list = []

for max_depth in range(1,15):

  accuracy_list = Kfold_crossvalidation(list, max_depth)

  depth_accuracy_list.append(np.mean(accuracy_list)*100)

#################### PLOTTING ###################################################################################

plt.figure(figsize = (14,8))
sns.set_context(context = 'notebook',font_scale = 2)
sns.set_style(style = 'whitegrid')
sns.lineplot(x = np.arange(1,15), y = depth_accuracy_list)
plt.title("Variation of KFold Accuracy with Max Depth")
plt.xlabel("Maximum Depth")
plt.ylabel("Accuracy in %")

#### NOT:  MAXIMUM ACCURACY OCCURS FOR DEPTH = 12 , ACCURACY = 95.74%

############################  THIRD QUESTION ############################################################
### PERFORMS PRUNING AND SAVES THE BEST PRUNED TREE UNTIL ACCURACY DECREASES

full_tree = build_regression_tree(train, 12, 1)
benchmark = evaluate_fulltree(full_tree)


global temp_tree
global tree_performance_list
i = 0
while(True):
    
    i+=1
    
    tree_performance_list = []

    temp_tree = full_tree

    prune_step(temp_tree, None, None)
    
    if(find_best_prune(tree_performance_list)[0] > benchmark):
        
        full_tree = find_best_prune(tree_performance_list)[1]
        benchmark = find_best_prune(tree_performance_list)[0]
        
    else:

        full_tree = build_regression_tree(train, 12, 1)
        break

############################### FOURTH QUESTION #################################################
### PRINTING THE BEST PRUNED TREE
### THE TREE IS STORED IN THE VARIABLE full_tree

print_tree(full_tree)

########################### FIFTH QUESTION #################################################

#Report ATTACHED

########################## PRINTING RESULT REPORT #########################################

root  = build_regression_tree(train + validation , 12, 1)
y_pred = []
y_test = []

a = test + validation

for sample in a:
  y_pred.append(predict(root,sample))
  y_test.append(sample["Deaths"])

confusion_matrix_report(y_test, y_pred)

